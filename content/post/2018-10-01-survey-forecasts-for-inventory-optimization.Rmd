---
title: 'Survey on Probabilistic Forecasts for Inventory Optimization'
author: Tim Radtke
date: '2018-11-18'
slug: survey-forecasts-for-inventory-optimization
categories:
  - forecasting
tags:
  - time series
  - forecasting
  - inventory optimization
  - optimal control
  - replenishment
  - survey
output:
  blogdown::html_page
---

Rob J. Hyndman in [Why are some things easier to forecast than others?](https://robjhyndman.com/hyndsight/hardforecasts/):

> Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts are no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.

## Introduction

With the following document I aim to collect and organize ideas and resources on the topic of forecasts and optimization methods for the replenishment of products as it occurs in ecommerce companies, for example. This document can by no means be a full representation of all ideas and methods that are available, and it is in particular going to lack in terms of overview over state-of-the-art operational research. Yet I hope that it can serve a fellow data scientist as an introduction to challenges and opportunities in forecasting sales of products for the purpose of optimizing the replenishment and inventory management of those.

Keep in mind that my own experiences and previous tasks govern the topics that I will highlight in the following, and that the importance of these topics might vary in your application. It remains your task to find out which of the points you most urgently need to address.

Let me start with some frustrations that will guide the remainder of the post.

## Frustration

You know what’s nice? Aggregated data. Like when you forecast electricity demand of a city, or the daily sales of an entire company. Because the value you forecast is the sum of many individual time series, individual errors get lost. If the trend of a single product drops abruptly, you might not even notice because a different product picks up in sales. You might be able to find daily or yearly seasonality that is sustained for years. You have years of historical data.

You might not have this luxury when your task is to forecast the sales of individual [stock keeping units (SKUs)](https://en.wikipedia.org/wiki/Stock_keeping_unit). Any given product might have been introduced two weeks ago and thus lack historical data. A product’s sales might skyrocket for a few days due to a promotion you were not informed about. They might increase because the brand is more heavily pushed across the website. A product might develop what looks like a yearly seasonality in one year, but then fail to realize it the next year because a close substitute is pushed instead. If we go by [Rob J. Hyndman’s three indicators of “Why are some things easier to forecast than others?”](https://robjhyndman.com/hyndsight/hardforecasts/), we see that it might be hard to forecast individual SKUs:

> 1) how well we understand the factors that contribute to it;
> 2) how much data are available;
> 3) whether the forecasts can affect the thing we are trying to forecast.

For a given SKU, we might not understand the factors driving the sales (e.g., incomplete information about promotions), we might not have enough data due to a recent launch, and the forecasts can even influence future importance and promotions of the product due to purchasing decisions (if the company bought a lot, it will try to sell a lot).

Interestingly, or frustatingly, these points remain mostly unaddressed by what forecasting method you choose. You might be able to address point 2) for an individual product if you have more products to compare it to. Point 3) brings us into the realm of reinforcement learning. And point 1) asks you to talk to your colleagues to understand their processes as much as possible; or to enforce processes to make contributing factors known. This is not what they teach you in school, but it can be the most valuable thing to do.

What I want to say is: You can do everything correct, and still be wrong. In that case, you need to hope that you’re evaluated on aggregate instead of individual time series, so that the errors are smoothed out.

## Problem Setting

I now define a rough setting that might or might not be similar to your situation. However it will make us focus on specific topics in the wide field of forecasting that are generally relevant to forecasting for inventory management.

### Available Data

Suppose you are tasked to provide forecasts for SKUs $i \in { 1, ..., n}$, where depending on the business model of your company $n \gg 1000$ is not unrealistic. Think Amazon, not Apple (or if Apple, then the accessories online shop, not iPhone). Let $y_{i,t}$ be the sales of product $i$ at time $t$ (e.g., daily, or weekly). Then $y_{i,t} \in \{0, 1, 2, 3, ...\}$, i.e. non-negative and discrete.

The latter assumption stems from the fact that the sales of a product can of course not be negative. This assumption can be neglected when observations are much larger than 0 and appear continuous (again, an advantage of aggregate time series); but if you're dealing with the following time series, a normal distribution assumption can give you nonsense forecasts (you're wasting probability on impossible outcomes):

```{r, message = FALSE, warning = FALSE, fig.cap = "Forecast with normal distribution for a product with discrete sales."}
library(ggplot2)
library(tidyr)
library(dplyr)
library(forecast)

set.seed(58295)
sales <- ts(rpois(5*7, 0.5), frequency = 7)
# as a very simple example, produce forecasts using 
# the mean forecast with normal distribution assumption
# see ?meanf
autoplot(forecast::meanf(sales)) + theme_bw()
```

While the `meanf()` model with its i.i.d. assumption produces a fine mean prediction given the simple data generating model, the prediction intervals make it looks as if -0.5 is a plausible future value due to the inappropriate assumption of a normal distribution. On the other hand, a discrete probability distribution as for example a Poisson distribution would do a better job assigning probabilitiy only to the non-negative integers.

If you have never seen a time series resembling the one above, then you might not draw a lot of value from the rest of this article. However, in case a few of your products look remotely like the graph, please continue to read.

That does not mean that all of the $n$ products must look like the graph above. The following two graphs could be part of the product portfolio, too. Which highlights the next point in our setting: A variety of behaviors across the $n$ products that you need to forecast.

```{r, echo = FALSE, fig.cap = "Two products with sales of quite different behavior."}
set.seed(58244)
# simulate a non-stationary series by hoping the ARIMA model returns something useful
pois_mean <- arima.sim(list(order = c(0,1,1), ma = c(0.7)), n = 200)
non_stationary <- ts(rpois(200, ifelse(pois_mean < 0, 0, pois_mean)), 
                frequency = 7)

# simulate a zero-inflated Poisson distribution to visualize
# intermittent demand
rp <- rpois(200,0.5)
rb <- purrr::rbernoulli(200, 0.3)
intermittent <- ts(rp * rb, frequency = 7)

example_series <- data.frame(date = seq(as.Date("2018-10-01"), 
                                                length.out = 200, by = 1),
           demand_type = rep(c("non_stationary", "intermittent"), 
                             each = 200),
           sales = c(non_stationary, intermittent))
ggplot(example_series, aes(x = date, y = sales)) +
  geom_line() +
  facet_wrap(~demand_type, scales = "free") +
  theme_bw()
```

This is an important point: If all products produced similar series, it would be much easier to find one optimal strategy to forecast them. But if one is zero-inflated and another is non-stationary with observations between zero and 100, then one size of model might not fit all products.

Suppose further that [out-of-stock events](https://en.wikipedia.org/wiki/Stockout) are possible and lead to "holes" in your historical data. Furthermore, depending on your company's setup, you might not have data about historical stockouts but only observe series of zero sales in your time series. Whether or not the observed demand is zero because the actual demand is zero or because the supply was zero, you might not know if you don't have historical stock information available.^[Again, it can be a valuable contribution of you as data scientist to not focus on forecasts first, but to introduce processes that keep track of historical stock movements as well as possible to bring clarity into which products ran out-of-stock and which had zero demand.] That means that you might need to infer from your observed demand whether a series of zeroes is likely to be a stockout event or not.

```{r, echo = FALSE, fig.cap = "At least one product with stockout. The second one... maybe?"}
non_stationary_oos <- non_stationary
non_stationary_oos[55:65] <- 0
example_series <- data.frame(date = seq(as.Date("2018-10-01"), 
                                                length.out = 200, by = 1),
           demand_type = rep(c("non_stationary", "intermittent"), 
                             each = 200),
           sales = c(non_stationary_oos, intermittent))
ggplot(example_series, aes(x = date, y = sales)) +
  geom_line() +
  facet_wrap(~demand_type, scales = "free") +
  theme_bw()
```

Next, assume that there is a hierarchical/group structure across your products due to multiple colors, sizes, or taste of a product, due to brands or categories and parent categories. Imagine, for example, the books **department** on Amazon or in Dussmann^[Come to Berlin to see [it](https://www.kulturkaufhaus.de).]. You have **categories** such as *Literature & Fiction* and *Non-Fiction*, which carry books by an **author** such as Haruki Murakami, *Norwegian Wood* in one category, *What I Talk About When I Talk About Running* in the other. Both books might appear in different **languages** and by different **publishers** in paperback or hardcover **format**. All of the highlighted groups might or might not help us in predicting the sales of a product, and create relationships across separate SKUs that share one or many categories. Maybe all books in the fiction category see rising sales before the summer vacation. Maybe all books by Haruki Murakami experience a sales increase when he releases a new book. Maybe there are related authors whose books also start to sell better.

While the setting described here is supposed to allow for the existence of some feature $k$ with observations $x_{i,t}^k$ such as product-specific promotional events or the product's selling price (over time), hierarchies and groups such as the ones above enforce structure that can be informative beyond dummy variables. Suppose you are tasked to not only predict the sales of individual SKUs, but also those of the book categories (to plan overall category performance). Let's say that you only sell the two books by Murakami as both hardcover and paperback. Your SKU-level forecasts predict daily average sales of 1 and 5 for the  *Norwegian Wood* hardcover and paperback, as well as 0.2 and 2 for *What I Talk About When I Talk About Running*. The forecasts you produced for the Literature & Fiction category predict 5 sales per day, while you expect 2 sales per day in the Non-Fiction category. Overall, you expect to sell 6 books per day. 

If these were your forecasts, you would have inconsistencies across the hierarchy because the hierarchy was neglected when producing the forecasts. The forecasts for the Norwegian Wood SKUs should add up to the Literature & Fiction forecast, just as the forecasts for all SKUs should add up to the total books per day forecast. As we will see later, by enforcing this type of structure, you might be able to transfer information from the aggregated series down to the individual series; and the other way around.

### Forecast Goal

Before we start to delve into forecast methods, we must define how we expect the forecasts to be used once we have them. Traditionally, a manager would be happy to receive a (mean forecast) curve that indicates where things are going in the future. Maybe hard-coded safety stocks were added on top of the mean forecast to produce values for the buying department to replenish.

However, we are in a situation that allows us to trade off cost and benefit of a given product's service level. There are opportunity cost from a stockout (unfulfilled demand that does not choose a substitute product) as well as cost from stocking too many products (warehousing, perishable items, money tied up in stocks). If you are able to quantify these cost^[Again, take your responsibility as data scientist to make the cost quantifiable.], then it is possible to optimize the [service level](https://en.wikipedia.org/wiki/Service_level) that you want to reach for a given product.

If you follow the [link to Wikipedia's definition of the $\alpha$-service level](https://en.wikipedia.org/wiki/Service_level#α_service_level_(type_1)), it should be clear that you require the $\alpha$-quantile of your forecast's probability distribution for a given period to determine how much needs to be re-ordered to ensure an $\alpha$-service level. If your forecasts follow some simple normal distribution as shown above, then this does not add much complexity on top of producing a mean forecast. Once you move beyond using a normal distribution for everything, however, producing the probability distribution of future sales becomes more challenging.

This challenge, however, is what makes forecasts for inventory optimization, and for optimal control problems in general, so interesting and what has spurred a lot of research over the past years. Another well documented area is the forecast of electricity demand in which distributional forecasts are just as important to optimize the control of energy production.

## Probabilistic Forecasts

In the following we will explore how researchers have dealt with the many problems in demand forecasting mentioned so far. We will see that most methods fail to address all of the difficulties and opportunities that the area of demand forecasting for inventory control has to offer. Given the combined complexity of all topics, this is hardly surprising.

This is made worse by the fact that in a real dataset, some products will display one characteristic while other products show a different characteristic; some products might have intermittent demand, while others are fast moving products with seasonality and important exogeneous features. Consequently, a single method of those described in this survey might not always be sufficient.

We see a few options to address this point. The first option, and likely the simplest, would be to find the most relevant points in your data and to select a model that is able to address these reliably. In that case, it can be relevant to find summary statistics that describe the data well. A few useful summary statistics can be found in Figure 3 of [Seeger et al. (2016)](https://papers.nips.cc/paper/6313-bayesian-intermittent-demand-forecasting-for-large-inventories).

The second approach would be to build one all-encompassing model trained on all products. This model would hopefully be flexible and powerful enough to address all difficulties individually for every product while learning and sharing information across products. Teams at Amazon have shown the most promising steps in this direction, with [Flunkert et al. (2017)](https://arxiv.org/abs/1704.04110), [Wen et al. (2018)](https://arxiv.org/abs/1711.11053) and [Madeka et al. (2018)](https://drive.google.com/file/d/16-wIlZ3mcCxsWnOOVkdJMWAT4TIPVJiA/view).

Lastly, one could try to design a meta-learning approach that selects the most relevant model given the current product and time series, similarly to how the `auto.arima()` or `ets()` functions of the [`forecast` R package](http://pkg.robjhyndman.com/forecast/) try to find the optimal model specification for a given time series. Ideally, though, one would not be limited to a single class of models. Alternatively, one could for example train a model that automatically selects models for every product given the products' time series characteristics. An example of what this might look like is given in [Talagala et al. (2018)](https://robjhyndman.com/publications/fforms/).

Given these three rough options, we see that there is also a meta-meta-learning task to be solved given, for example, different data availability at different companies. We now explore models and approaches researchers have used to address the many characteristics of the forecasting for inventory control problem. We start with challenges given by how the forecasts are used in upstream processes, and then proceed to challenges given by characteristics of typical datasets of product demand.  

### Methods to Address the Characteristics of the Forecast Goal

In this survey, we are concerned with demand forecasts for individual products. These forecasts are then used in some upstream optimization process for optimal re-order decisions and inventory control. In the most general setting, we have different costs that arise from wrong forecasts and wrong inventory management. Too high forecasts and consequently too much stock leads to, for example, warehousing cost. On the other hand, some opportunity cost arises when stockouts occur because too low demand was forecast for a given product. While these cost can vary highly from company to company and product to product (compare for example books versus fresh fruits or dairy products that are perishable), they are typically not symmetric. If the opportunity cost from a stockout are larger, then we want to ensure that stockouts happen rarely. The algorithm for re-order decisions will trade off these costs and find an optimal service level for a given product. The service level $s \in (0,1)$ is essentially the probability with which a given product should be available. In the case of symmetric cost (too low forecasts are as costly as too high forecasts), the service level would equal $s = 0.5$. If we want to prevent costly stockouts, then the service level is increased to $s > 0.5$. We write the service level as the probability with which the actual demand is smaller than the forecasted demand, $s = P(Y \leq \hat{Y})$. 

From the last equation, the need for probabilistic demand forecasts is derived. For some time period $[L,L+S)$ in the future, we need to be able to solve the last equation for $\hat{Y} = \hat{Y}_{(L,S)} = \sum_{t = L}^{L+S-1}\hat{y}_{i,t}$. That is, we need to be able to quantify the entire distribution of total demand in the given future time period, $p(Y_{(L,S)}|I_T)$, where $I_T$ is all information available up to the forecast creation time $T$. We solve the equation for $\hat{Y}$ by computing the quantile $s$ of that distribution. Any model considered for providing forecasts is required to provide these quantiles, in some way or another.

Put differently, because we want to use the forecasts as input for the re-order decision algorithm, we have forecast the entire distribution of future demand, independent of what our data looks like.

#### Evaluation Metrics and Loss Functions

Before thinking about producing forecasts and comparing different approaches, let's decide how we will evaluate our forecasts. 

From the last point made in the problem setting, we know that we are not (solely) interested in mean forecasts but in distributional forecasts. Due to this, we cannot rely on the classical evaluation metrics to guide our model development. By classical evaluation metrics I mean those described in [chapter 3.4](https://otexts.org/fpp2/accuracy.html) of [Hyndman and Athanasopoulos' *Forecasting: Principles and Practice*](https://otexts.org/fpp2/): MAE, RMSE, MAPE, sMAPE, MASE all are not what we want because the *M* stands for *Mean* in all of them. The mean is not what we try to optimize for, though. These measures of course have their place; a comparative study of the measures can be found in [Hyndman and Koehler (2006)](https://robjhyndman.com/publications/another-look-at-measures-of-forecast-accuracy/P).

Instead, we need to evaluate how well the predicted distribution fits the distribution of realized values.

In [Snyder et al. (2012)](https://www.researchgate.net/publication/251527888), rely on the MASE (mean absolute percentage error) as introduced in [Hyndman and Koehler (2006)](https://robjhyndman.com/publications/another-look-at-measures-of-forecast-accuracy/P) to display a classical performance measure, while highlighting the importance of "distribution based scores". They "describe two criteria that can be used to measure the forecasting performance relative to the predictive distribution". The first is the *prediction likelihood score* (PLS) which describes the likelihood with which the heldout samples have been generated by the predictive distribution. Let $T$ be the end of the training sample, and $h$ be the number of timesteps that we predict ahead. Given all information $I_T$ up to and including time $T$, as well as the heldout future sales $y_{t+1}, ..., y_{t+h}$, the joint likelihood of the heldout series is given by $p(y_{t+1}, ..., y_{t+h} | I_t)$. The authors compute this likelihood as the product of one-step ahead likelihoods:

$$
p(y_{t+1}, ..., y_{t+h} | I_t) = \prod_{t = T+1}^{T+h} p(y_t|I_{t-1}^*),
$$

where $I_t^* = \{I_T, y_T, ..., y_t\}$ is the updated information including the all sales $y_t$ up to and including time $t$. Given that the authors exclusively use discrete probability distributions, $p(y_t|I_{t-1}^*)$ is the one-step ahead predictive probability mass function. Let's make this even more clear with an example. First, we sample from a Negative Binomial distribution to create the time series with train and test set. We ensure an overdispersed sample so that generally the Negative Binomial should be preferred over a Poisson distribution.

```{r, fig.cap = "Overdispersed samples from a Negative Binomial distribution."}
set.seed(58295)
sales <- rnbinom(5*7, mu = 2, size = 1)
sales_train <- sales[1:(4*7)]
sales_test <- sales[(4*7+1):(5*7)]
data.frame(days = 1:(5*7), sales = sales) %>%
  ggplot(aes(x = days, y = sales)) +
  geom_vline(aes(xintercept = 4*7+0.5), linetype = 2) +
  geom_point()
```

We can then iteratively fit and re-fit both a Poisson and a Negative Binomial distribution to the training data and compute the likelihood of the heldout sample. To ease the comparison, we show the log-PLS and compare the two models. Larger values are better.

```{r}
pls_nbinom <- pls_pois <- vector()

for(i in 1:length(sales_test)) {
  fit_pois <- glm(c(sales_train, sales_test[i-1])~1, family = poisson)
  fit_nbinom <- MASS::glm.nb(c(sales_train, sales_test[i-1])~1)
  pls_pois[i] <- dpois(sales_test[i], exp(fit_pois$coefficients))
  pls_nbinom[i] <- dnbinom(sales_test[i], 
                           mu = unique(fit_nbinom$fitted.values),
                           size = fit_nbinom$theta)
}

PLS_pois = log(prod(pls_pois))
PLS_nbinom = log(prod(pls_nbinom))
```

```{r, echo = FALSE}
data.frame(pois = PLS_pois, 
           nbinom = PLS_nbinom) %>%
  knitr::kable(digits = 1, 
               col.names = c("PLS Poisson", "PLS Negative Binomial"))
```

In this (admittedly small) example, the PLS would actually prefer the Poisson model over the Negative Binomial model! Both models produce the same mean prediction, so MASE and similar values could not distinguish between the two. But we would have expected the PLS to prefer the Negative Binomial for its overdispersion. Note that the training log-likelihood at the last step are `r round(logLik(fit_pois),1)` and `r round(logLik(fit_nbinom),1)` for Poisson and Negative Binomial respectively, displaying the advantage of the overdispersed model.

Snyder et al. also discuss the option to compute the PLS for the sum over the lead time period, $Y_T(h) = \sum_{t = T+1}^{T+h} y_t$, as $p(Y_T(h)|I_T)$. However, they do not specify in detail how the likelihood for the sum can be computed in practice. This is unfortunate since--as noted by Snyder et al.--the PLS for the sum is arguably the more relevant measure for the evaluation of forecasts that steer the re-order process; after all, we need to know the *total* demand over the lead time period to know whether we need to order and/or how much to order.

In our previous example with models that assume independent observations over time, it would be possible to compute the PLS of the sum:

$$
\sum_{i = 1}^n X_i \sim \text{Pois}(\sum_{i=1}^n \lambda_i) \quad \text{if for } i = 1,...,n, X_i \sim \text{Pois}(\lambda_i) \text{ independent}
$$

and

$$
\sum_{i = 1}^n X_i \sim \text{NB}(\sum_{i=1}^n r_i, p) \quad \text{if for } i = 1,...,n, X_i \sim \text{NB}(r_i,p) \text{ independent.}
$$

Given this, we can train the models on the first four weeks of observations and then compute the PLS of the sum for the heldout demand in the fifth week.

```{r}
fit_pois <- glm(sales_train~1, family = poisson)
fit_nbinom <- MASS::glm.nb(sales_train~1)
spls_pois <- dpois(sum(sales_test), 7*exp(fit_pois$coefficients))
spls_nbinom <- dnbinom(sum(sales_test), 
                         mu = 7*unique(fit_nbinom$fitted.values),
                         size = fit_nbinom$theta)
```

```{r, echo = FALSE}
data.frame(pois = spls_pois, 
           nbinom = spls_nbinom) %>%
  knitr::kable(digits = 3, 
               col.names = c("Sum PLS Poisson", "Sum PLS Negative Binomial"))
```

Again the Poisson model would (wrongly?) be preferred. Snyder et al., however, compute the PLS for many different products in their data sets, and then average the PLS over all products to evaluate the models based on many different realizations. This usual comparison on a heldout test set is recommended by the authors to evaluate the performance of different models across a set of many SKUs. However, they argue for the use of information criteria such as AIC and BIC on training set to evaluate for a given product which model or model parameters are to be preferred. They argue that this can be especially advantageous in their case as they only have a test set of six observations for a given product. In the above example, the AIC would have (correctly?) selected the Negative Binomial model to perform the predictions given that the training set happened to have much more overdispersion.

While [Seeger et al. (2016)](https://papers.nips.cc/paper/6313-bayesian-intermittent-demand-forecasting-for-large-inventories) and [Flunkert et al. (2017)](https://arxiv.org/abs/1704.04110) compare their model against the `NegBin` model introduced in Snyder et al. (2012), they do so using a different evaluation metric, *quantile risk*. The quantile risk evaluates the model distributions at a given quantile after a given lead time. Given that a quantile at a certain lead time is usually the input for the re-order decision, it makes sense to evaluate how well the models are able to predict the relevant quantiles. Given that there is no closed form for the models the authors use, the quantiles are computed by way of generating--in this case 100--sample paths over the forecast horizon. 

Instead of a simple period of $h$ steps into the future, Seeger et al. consider the more general definition of a "span" $[L,L+S)$, where $L$ is some number of time steps into the future, and $S$ defines the length of the span. Where Snyder et al. had $Y_{i,T}(h) = \sum_{t = T+1}^{T+h}y_{i,t}$ for a given product $i$, we now consider the sum of the actuals $Y_{i,(L,S)} = \sum_{t = L}^{L+S-1} y_{i,t}$ as well as the sum of the predictions $\hat{Y}_{i,(L,S)} = \sum_{t = L}^{L+S-1} \hat{y}_{i,t}$.^[Note that Seeger et al. exclude stockout observations in their calculation.] Given that we generate $N$ sample paths from the model, we have a set of $N$ observations $\{\hat{Y}_{i,(L,S)}^1, ..., \hat{Y}_{i,(L,S)}^N\}$. Over this set, we can compute the quantile $\rho$ where $\rho \in (0,1)$ and write the resulting *predicted* quantile of product $i$ for the *total* demand over the span as $\hat{Y}_{i,(L,S)}^{\rho}$. To evaluate this quantile, the *$\rho$-quantile loss* is defined for a single product as $L(z,\hat{z})=2(z-\hat{z})(\rho \text{I}_{\{ z > \hat{z} \}} - (1-\rho) \text{I}_{\{z \leq \hat{z}\}})$, where $\text{I}$ is the indicator function. The $P(\rho \cdot 100)$ quantile risk over all $n$ products for $[L,L+S)$ is then defined as

$$
R^{\rho}_{(L,S)} = \frac{1}{n} \sum_{i=1}^{n} L\big(Y_{i,(L,S)}, \hat{Y}^{\rho}_{i,(L,S)}\big).
$$

As stressed by Seeger et al., the $\rho$-quantile minimizes $E_Y[L_{\rho}(Y,\hat{Y})]$, just like the sample mean minimizes $E_Y[(Y - \hat{Y})^2]$. Also note that for $\rho = 0.5$, the sample median minimizes the quantile loss, which in this case is equal to the mean absolute error (MAE) mentioned above: $L_{0.5}(Y, \hat{Y}) = 2 \cdot 0.5 \cdot| Y - \hat{Y}| = | Y - \hat{Y}|$. The last case highlights that the $2$ in the definition of the quantile loss is important to scale the loss back to the original unit (here, units of products). Thus we can compare for a given product the quantile loss for different values of $\rho$; ideally, the loss would be 0 at all $\rho$, but even when positive, the loss should ideally be the same at all values of $\rho$ to be equally well calibrated at all parts of the distribution. For example, $L_{0.9} \gg L_{0.5}$ would indicate a worse calibration of the predictive distribution at the upper tail.

Furthermore, if we have for every product $i$ a specific quantile $\rho_i$ that we need to optimize for (i.e., every product has its optimal service level), it means that we can use a different quantile for every product in the definition of the quantile risk $R_{(L,S)}$. This knowledge is of course specific to your company and use case. It also goes beyond the $P50$ and $P90$ quantile risks with fixed $\rho = 0.5$ and $\rho = 0.9$ for all products used in Seeger et al. (2016).

The last point highlights that the quantile risk used by Seeger et al. (2016) should be preferable over the prediction likelihood score used by Snyder et al. (2012) to evaluate the forecast performance for inventory optimization: The prediction likelihood score does not incorporate information from subsequent steps in replenishment automation, while the quantile risk comes as close as possible to where the money is.^[Biggest difference between industry and academia: You try to measure performance in units of dollars, not as mean absolute scaled error or similar.]

That there is value in going as close to where the money is when optimizing forecasts, is demonstrated by another team at Amazon: [Wen et al. (2018)](https://arxiv.org/abs/1711.11053) use the quantile loss not only as an evaluation metric--as done by Seeger et al. (2016) and Flunkert et al. (2017)--but incorporate the quantile loss directly into their training scheme as the loss function. While the previously mentioned papers all train their models on some (penalized) likelihood loss function and then evaluate models on quantile loss, [Wen et al. (2018)](https://arxiv.org/abs/1711.11053) aim to achieve better performance on the evaluation metric by optimizing the same loss during training. They consequently perform a form of quantile regression in the tradition of [Koenker and Bassett (1978)](https://pdfs.semanticscholar.org/a3cd/bfbba2ef3ce285980edc1213a4ac56f05bb1.pdf) and [Koenker (2005)](https://www.cambridge.org/core/books/quantile-regression/C18AE7BCF3EC43C16937390D44A328B1). As such, their model directly outputs several quantiles for different time periods into the future, instead of generating sample paths first to estimate the empirical distribution.

However, note what exactly is happening: In the previous papers (Snyder et al., Seeger et al., Flunkert et al.), the models are able to generate sample paths of the future demand. Thereby, the models can predict both the *marginal* demand distribution of $y_t$ at time step $t$ in the future, as well as the distribution of the total demand in any period $[L,L+S)$ in the future---by simply summing over the samples between time steps $L$ and $L+S-1$. The latter is possible as the sample paths describe the *joint* distribution of future time periods. Being able to describe the total period demand is crucial for probabilistic demand forecasting used for buying decisions and inventory management.

In Wen et al. (2018), on the other hand, only quantiles of the marginal demand $y_t$ at time step $t$ are returned. And since quantiles are in general not additive, their model fails to provide estimates of the joint distribution of time spans in the future. Their conclusion addresses this shortcoming and leaves it for future work. Indeed, the team tries to address the point in the subsequent [Madeka et al. (2018)](https://drive.google.com/file/d/16-wIlZ3mcCxsWnOOVkdJMWAT4TIPVJiA/view). However, the authors have to do so by employing models that do not use the quantile loss as loss function.

In conclusion: Try to optimize the output of the forecast according to how the forecast will be used in upstream processes. In the case of inventory optimization, you should attach stockout and warehousing costs to each product. Since these costs are not symmetric, forecasts should be evaluated by quantile losses rather than mean-oriented metrics. Alternatively, measure the performance of your models in dollar terms. 

#### How to Produce a Probabilistic Forecast

As usual, it is not difficult to produce *a* probabilistic forecasts, but it is more involved to produce a probabilistic forecast that will be *satisfying in terms of the evaluation metrics* described above.

To make a simple example, consider again the simulated 7 weeks sample of daily demands provided by i.i.d. samples from a Negative Binomial distribution. Let's say we need to keep a service level $s = 0.9$, and we are interested in the total period demand of days $[29,35]$. Thus we want to provide the $90\%$ quantile of $p(Y_{(29,7)}|I_{28})$ in the notation from the beginning of this chapter. 

Using the static negative binomial model fitted above, this is as simple as the following few lines of code:

```{r}
fit_nbinom <- MASS::glm.nb(sales_train~1)
q90_nbinom <- qnbinom(0.9, 
                      mu = 7*unique(fit_nbinom$fitted.values),
                      size = 7*fit_nbinom$theta)
# 90% quantile of the total demand in week 5
q90_nbinom
```

Here, this is easy to do given the static i.i.d. nature of the model. A closed form solution as in the previous example is usually not available as soon as we move into slightly more complex models. Then, the only solution is often to use *sample paths*. That means that we fit our model, and then use the fitted model to generate samples of future demand at every time step into the future. This gives one sample path $\{\hat{y}_{T+1}^k, ..., \hat{y}_{L}^k,... \hat{y}_{L+S-1}^k\}$. This process is then repeated many times, for $1 \leq k \leq K$, and $K \geq 100$, until we can compute quantiles over these sample paths. These sample paths approximate the joint distribution $p(y_{T+1}, ..., y_{L},... y_{L+S-1}|I_T)$. Since we are interested in $p(Y_{(L,S)}|I_T) = p(y_{L} + ... + y_{L+S-1} |I_T)$, we would first sum $\hat{y}_{L}^k,... \hat{y}_{L+S-1}^k$ for every $k$ to get $\hat{Y}_{(L,S)}^k$, and then compute the $s$ quantile $\hat{Y}_{(L,S)}^s$ over $\{\hat{Y}_{(L,S)}^1, ..., \hat{Y}_{(L,S)}^K\}$. Using the same model as before, we can demonstrate this quickly.

```{r}
fit_nbinom <- MASS::glm.nb(sales_train~1)
# since the model is static, we can take a shortcut when drawing samples
sample_paths <- matrix(rnbinom(7*1000,
                               mu = unique(fit_nbinom$fitted.values),
                               size = fit_nbinom$theta),
                       ncol = 1000)
q90_nbinom_sampled <- quantile(colSums(sample_paths), 0.9)

fit_nbinom_total <- MASS::glm.nb(colSums(sample_paths)~1)
q90_nbinom_sampled
```

```{r, echo = FALSE, fig.cap = "The graph shows the cumulative demand over the seven time steps in the future for 1000 simulated paths. The overlapping paths make the probability of outcomes visible through the resulting transparency. The actually realized demand is the orange line. Black points indicate the 90% quantile at every step."}
sample_path_df <- as.data.frame(sample_paths)
sample_path_df$index <- 1:7
actuals <- data.frame(index = 1:7, actual = sales_test)
actuals$actual_cum <- cumsum(actuals$actual)
sample_path_df_long <- tidyr::gather(sample_path_df,
                                     sample_index, sample, -index) %>%
  dplyr::group_by(sample_index) %>%
  dplyr::mutate(sample_cum = cumsum(sample)) %>%
  dplyr::ungroup()
sample_path_quantile <- sample_path_df_long %>%
  group_by(index) %>%
  summarize(q90_cum = quantile(sample_cum, 0.9)) %>%
  ungroup

ggplot() +
  geom_line(aes(x = index, y = sample_cum, group = sample_index), 
            data = filter(sample_path_df_long,
                          sample_index %in% paste0("V", 1:1000)), 
            alpha = 0.05) +
  geom_point(aes(x = index, y = q90_cum), 
             data = sample_path_quantile) +
  geom_line(aes(x = index, y = actual_cum), size = 1,
            data = actuals, col = "darkorange") +
  labs(y = "Cumulative Demand", x = "Index",
       title = "Cumulative Sample Paths from 1000 Simulations") +
  theme_bw()
```

Again, the static model simplifies the computation. For example, we have that $p(y_{T+2}|I_T, y_{T+1}) = p(y_{T+2}|I_T)$. This is, however, generally not the case. We often deal with *auto-regressive* models where we can only compute the sample $\hat{y}_{T+2}$ after we have computed $\hat{y}_{T+1}$. Since this limits the extend with which the simulations can be parallelized, it hampers the computation time potentially drastically. For a very nice example of this problem outside of demand forecasts, consider the original [WaveNet paper](https://arxiv.org/abs/1609.03499), as well as the [parallelized WaveNet paper](https://arxiv.org/abs/1711.10433):

> The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting.

Additionally, sample paths come with a cost on storage. For any given product, you need to compute hundreds of sample paths. To compute the median demand, 100 sample paths might be sufficient to achieve a satisfactory estimate. However, the larger (or smaller) the quantiles are, the more difficult it is to estimate them without being affected to outliers. Take for example the 90% quantile. If you estimate it based on a set of 10 sample paths, then the value of the quantile will be driven by the two largest realizations of the samples. With 100 sample paths, the 90% quantile is largely driven by 11 sample realizations, and with 1000 sample paths, it's largely driven by 101 sample realizations.

In the graph below, we have repeatedly estimated the 90% quantile for samples drawn from a Negative Binomial distribution `rnbinom(n = n, mu = 50, size = 50)`, which is a distribution with mean 50 and variance 100. We let the number of samples based on which we estimate the quantile vary, with $n \in \{10, 100, 1000, 10000\}$. For every $n$, we repeat the estimation $10000$-times. The number of samples is obviously insuffient at $n = 10$, leading to enormous variation in the quantile estimates. At $n = 100$, the median estimate is correct, and at $n = 1000$, the entire boxplot is within 5 units.

Thus there is a clear trade off between the precision of our quantile estimates for the reorder decisions, and the storage and time consumption of the sampling process. Besides the potentially slow sampling process, as mentioned for the WaveNet papers above, one would potentially like to store the sample paths to allow for a later computation of different quantiles or different time spans of demand. Note that we cannot compute the 90% quantile of the total demand for the period $[10,50)$ based on the cumulative quantiles for every time step between timestep $0$ and timestep $50$ since quantiles are not additive. In contrast, one could of course compute the mean based on the mean at each timestep. Therefore, if we want or have to to preserve some flexibility in terms of the quantiles we compute later on (or if we wanted to aggregate across product categories later on, for example), we need to store for every SKU potentially hundreds of thousands of samples.

```{r, echo = FALSE, fig.cap = "We repeat the estimation of the 90% quantile based on different number of sample paths respectively 10000 times. The resulting distribution of estimated 90% quantiles is shown in the graph. The horizontal line indicates the true 90% quantile of the underlying Negative Binomial distribution."}
set.seed(748295)
m10 <- matrix(rnbinom(10*10000, mu = 50, size = 50), ncol = 10000)
m100 <- matrix(rnbinom(100*10000, mu = 50, size = 50), ncol = 10000)
m1000 <- matrix(rnbinom(1000*10000, mu = 50, size = 50), ncol = 10000)
m10000 <- matrix(rnbinom(10000*10000, mu = 50, size = 50), ncol = 10000)

m10q90 <- apply(m10, 2, quantile, 0.9)
m100q90 <- apply(m100, 2, quantile, 0.9)
m1000q90 <- apply(m1000, 2, quantile, 0.9)
m10000q90 <- apply(m10000, 2, quantile, 0.9)

res <- data.frame(
  sampled_quantiles = c(m10q90, m100q90, m1000q90, m10000q90),
  n_samples = c(rep("10", 10000), rep("100", 10000),
                rep("1000", 10000), rep("10000", 10000))
)

res %>%
  ggplot(aes(x = n_samples)) +
  geom_hline(aes(yintercept = qnbinom(0.9, mu = 50, size = 50)),
             linetype = 1, color = "orange", size = 1) +
  geom_boxplot(aes(y = sampled_quantiles)) +
  theme_bw() +
  labs(x = "Number of Samples", y = "Estimated 90% Quantiles",
       title = "Precision of Quantile Estimates")
```

- At this point it should be clear to the reader why we need probabilistic forecasts, and what they are

- Snyder et al.
- Bayesian Modeling of Groups of Count Time Series
- Amazon 1
- Amazon 2
- Amazon ICML 1
- Amazon ICML 2
- `smooth` package
- Hierarchical Probabilistic Forecasts
- Some papers on energy probabilistic forecasts
- Z

Next, papers that produce/highlight probabilistic forecasts.

Show the different time horizons of papers. 

Show how Amazon highlights robustness and scalability in their descriptions.

## Approaches to (Probabilistic) Demand Forecasts for Inventory Optimization

The following chapter will guide you through different approaches suggested in the literature to deal with one or many of the difficulties and opportunities in demand forecasting. This includes different approaches to probabilistic forecasting, as well as different approaches to the data setting described above. Both are inherently linked in the fact that we need to model the distribution in a way that fits the data.

### Non-negative Predictive Distributions

Once it is clear that you will have to generate probabilistic demand forecasts for individual SKUs, and you see a historical demand similar to the one in Figure 1, it is clear both that you need to pick some predictive distribution family and that the general approach of using a normal distribution can fail you. For that reason, most authors discussing probabilistic demand forecasts pick from a set of non-negative, discrete (count) distribution families.

For a general overview of count distributions, we refer to [Cameron and Trivedi (2013)](http://faculty.econ.ucdavis.edu/faculty/cameron/racd2/) and [Zeileis et al. (2017)](https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf).

For a good, but by now somewhat old, survey on modeling demand using non-negative, discrete distributions, consider [Snyder et al. (2012)](https://www.researchgate.net/publication/251527888). While they discuss different ways of modeling the mean of the distribution (either static or dynamic), these are all used in combination with a count distribution. Here, the authors compare the performance of the [Poisson](Hurdle Shifted Poisson), the [Negative Binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution), and the [Hurdle Shifted Poisson](https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf) distribution. The latter distribution in particular is used to address the small counts and zeroes often observed in demand datasets. In their evaluation, the Negative Binomial distribution comes in ahead of the other distributions. For a discussion of why the Poisson distribution can be overly restrictive and the Negative Binomial a good alternative, also refer to [a previous blog post of mine](https://minimizeregret.com/post/2018/01/04/understanding-the-negative-binomial-distribution/).

Thus it might be unsurprising to see the Negative Binomial as the distribution of choice in other papers as well. [Chapados (2014)](http://proceedings.mlr.press/v32/chapados14) uses the zero-inflated Negative Binomial distribution to model the demand $y_t$ in a hierarchical Bayesian model that goes beyond the univariate formulation used in [Snyder et al. (2012)](https://www.researchgate.net/publication/251527888). [Seeger et al. (2016)](https://papers.nips.cc/paper/6313-bayesian-intermittent-demand-forecasting-for-large-inventories) also build on top of Snyder et al. (2012), however introduce a multi-stage Poisson likelihood similar in idea to zero-inflated and hurdle models, as it accounts for the large occurence of zeroes and ones in the data that cannot be modeled well by a Poisson. In a later paper, [Flunkert et al. (2017)]((https://arxiv.org/abs/1704.04110)) use a Negative Binomial distribution to generate sample paths; the parameters of the Negative Binomial in this case being set by their LSTM model, which iteratively reads the samples of the Negative Binomial to then produce the parameters for the next time step. For more on this kind of combination of LSTM and predictive distribution, also see [Graves (2014)](https://arxiv.org/abs/1308.0850) and [Bishop (1994)](https://www.microsoft.com/en-us/research/publication/mixture-density-networks/).

Deeper discussion of the models besides predictive distribution? Then also include `smooth` package and `tscount`? 

[Madeka et al. (2018)](https://milets18.github.io/papers/milets18_paper_14.pdf) try to move beyond this approach of the distributional assumption employed by the other papers to create sample paths. Instead, they evaluate Generative Adversarial Networks and Variational Autoencoders, however without satisfactory results so far.

### Detection and Treatment of Stockout Events

In all of the discussion so far, we try to forecast the consumer demand for specific products. Stockout events are time periods during which the true consumer demand cannot be observed as the products are not available for purchase. Thus, stockout events create missing observations in our time series.

While missing observations are a classical problem in all of statistics and machine learning, stockout events are a little bit meaner yet. 

First, do you know when a products had stockout events? Have these events been tracked by, for example, historical stock movements and are thus available in your data warehouse? If not, you have two problems. Not only do you need to *treat* stockout observations, you also need to *detect* stockout observations. As illustrated in Figure 3 above, this can be more or less straightforward. A sequence of zero observations in a fast moving product is very suspicous and easily identified. For a slow moving product, however, a series of zeroes can be a representation of the actual demand. If you don't keep track of this already, the best approach is likely to start doing so. Otherwise you'll have to find [other means of detecting overly long sequences of zeroes](https://minimizeregret.com/post/2017/10/22/detection-abnormal-zero-sequences-count-time-series/) that should not be predicted into the future.

Note that zero-inflated models are not supposed to account for zeroes created by stockout events. The decision in [Chapados (2014)](http://proceedings.mlr.press/v32/chapados14) to use a zero-inflated Negative Binomial as observational model and a Negative Binomial as predictive model essentially treats all excess zeroes beyond those modeled by the Negative Binomial as stockout events and not as consumer signal, thus not forecasting those zeroes into the future. While this might work well for fast-moving items that don't have zeroes besides those from stockout events, their assumption appears problematic for items with intermittent demand where excess zeroes appear naturally. Here, their assumption could lead to upward bias in the forecast.

In contrast, the usual problem generated by stockout events is downward bias in forecasts. When stockout events and their sequences of zeroes go untreated, they hide positive demand. The model could produce to conservative forecasts as a result, which in the worst case produces more stockout periods, and so on. [Flunkert et al. (2017)](https://arxiv.org/abs/1704.04110) describe it as follows:

> Not explicitly modeling such missing observations (e.g. by assuming that the observed sales correspond to the demand even when an item is out of stock), can, in the best case, lead to systematic forecast underbias, and, in a worst case in the larger supply chain context, can lead to a disastrous downward spiral where an out-of- stock situation leads to a lower demand forecast, lower re-ordering and more out-of-stock-situations.

In their case, due to the auto-regressive nature of their LSTM combined with the Negative Binomial predictive distribution, they can draw a sample from the conditional distribution at time $t$ to interpolate the missing observation $y_t$ during training, however exclude the observation from the likelihood computation used to fit the LSTM. The same approach has been taken with the team's earlier model by [Seeger et al. (2016)](https://papers.nips.cc/paper/6313-bayesian-intermittent-demand-forecasting-for-large-inventories).

### Univariate or Multi-Variate Models, Hierarchy, and Exogenous Features

The easiest way to forecast a product's time series $Y_{i,T} = \{y_{i,1}, y_{i,2}, ..., y_{i,T}\}$, is to base the forecast $\hat{y}_{i,t+h}$ exclusively on the product's past observations $Y_{i,T}$. In the examples above, we try to estimate $E[y_{i,t+h}|Y_{i,t}]$ with $h\geq1$. The same happens if you take the `forecast()` function from the `forecast` R package and apply it to a product's time series. [Snyder et al. (2012)](https://www.researchgate.net/publication/251527888) also forecast a product's future sales based only on each product's individual sales.

These methods exist as there are many applications where we have to forecast a time series with the time series being the only available data, and where the forecast works fairly well based on the historical data alone. If it succeeds, then it does due to repeating patterns which only depend on previous data.

In the inventory optimization setting, however, we have reason to suspect that there is other data available that can help predict a product's future sales.

First, there can be exogenous features such as a product's promotional events or its selling price, seasonality-modeling Fourier series, or time-specific features such as holidays. These can of course be used in any regression-like model including in particular auto-regressive models. Both [Seeger et al. (2016)](https://papers.nips.cc/paper/6313-bayesian-intermittent-demand-forecasting-for-large-inventories) and [Chapados (2014)](http://proceedings.mlr.press/v32/chapados14) use (auto-regressive) state space models that allow for the inclusion of such features to model exogenous shocks. 

Second, we have not just one, but many parallel time series available. This opens the door for multi-variate forecasting approaches such as vector autoregressive (VAR) or vector autoregressive moving average (VARMA) models (e.g., [Lütkepohl, 2005](https://www.springer.com/us/book/9783540401728)). A longitudinal regression approach, potentially with auto-regressive components, might be applicable as well given that we observe several "individuals" over time. [Chapados (2014)](http://proceedings.mlr.press/v32/chapados14) builds a single model trained on all products that predicts all products. Similarly, when predicting the sales of different stores into the future, the most successful approaches of the Rossmann Store Sales competition were models trained on all products at once (see [here](https://www.kaggle.com/c/rossmann-store-sales/discussion/18024) and [here](https://www.kaggle.com/c/rossmann-store-sales/discussion/17974)). Team Neokami of the latter example explicitly modeled the similarity between the stores using an embedding layer in their neural network model ([Guo and Berkhahn, 2016](https://arxiv.org/pdf/1604.06737.pdf)).

While the previous approaches deal with data sets where not much information is available on how individual time series relate to each other, we would usually have some grouped or hierarchical structure for products given by the categories in which they are sold. What this can look like was described in the problem setting above. [Chapados (2014)](http://proceedings.mlr.press/v32/chapados14) builds what is called a hierarchical model ([Gelman and Hill, 2007](http://www.stat.columbia.edu/~gelman/arm/)), though his data sets really just allows for different groups of the products (e.g., same products at different stores). Yet it enables to share the information from several stores for the same product, which ideally enables better generalization during prediction.

## References

A. Colin Cameron and Pravin K. Trivedi (2013). [Regression Analysis of Count Data](http://faculty.econ.ucdavis.edu/faculty/cameron/racd2/). Econometric Society Monograph No. 53, Cambridge University Press, Cambridge.

Christopher Bishop (1994). [Mixture Density Networks](https://www.microsoft.com/en-us/research/publication/mixture-density-networks/#!related_info). Technical Report, Microsoft.

Nicolas Chapados (2014). [Effective Bayesian Modeling of Groups of Related Count Time Series](http://proceedings.mlr.press/v32/chapados14). Proceedings of the 31st International Conference on Machine Learning, PMLR 32(2):1395-1403, 2014.

Valentin Flunkert, David Salinas, Jan Gasthaus (2017). [DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks](https://arxiv.org/abs/1704.04110). arXiv preprint arXiv:1704.04110

Andrew Gelman, Jennifer Hill (2007). [Data Analysis Using Regression and Multilevel/Hierarchical Models](http://www.stat.columbia.edu/~gelman/arm/). Cambridge University Press, 2007.

Alex Graves (2014). [Generating Sequences With Recurrent Neural Networks](https://arxiv.org/abs/1308.0850). arXiv preprint arXiv:1308.0850

Cheng Guo, Felix Berkhahn (2016). [Entity Embeddings of Categorical Variables](https://arxiv.org/abs/1604.06737). arXiv preprint arXiv:1604.06737

Rob J Hyndman (2012). [Why are some things easier to forecast than others?](https://robjhyndman.com/hyndsight/hardforecasts/). Blog post.

Rob J Hyndman, George Athanasopoulos (2018). [Forecasting: Principles and Practice](https://otexts.org/fpp2/). Online version.

Rob J Hyndman, Anne B Koehler (2006). [Another look at measures of forecast accuracy](https://robjhyndman.com/publications/another-look-at-measures-of-forecast-accuracy/). International Journal of Forecasting 22(4), 679-688.

Roger Koenker, Gilbert Bassett (1978). [Regression Quantiles](https://pdfs.semanticscholar.org/a3cd/bfbba2ef3ce285980edc1213a4ac56f05bb1.pdf). Econometrica, Vol. 46, No. 1, pp. 33-50.

Roger Koenker (2005). [Quantile Regression](https://www.cambridge.org/core/books/quantile-regression/C18AE7BCF3EC43C16937390D44A328B1). Cambridge University Press.

Helmut Lütkepohl (2005). [New Introduction to Multiple Time Series Analysis](https://www.springer.com/us/book/9783540401728). Springer Verlag Berlin Heidelberg.

Dhruv Madeka, Lucas Swiniarski, Dean Foster, Leo Razoumov, Kari Torkkola, Ruofeng Wen (2018). [Sample Path Generation for Probabilistic Demand Forecasting](https://drive.google.com/file/d/16-wIlZ3mcCxsWnOOVkdJMWAT4TIPVJiA/view). ICML 2018 Workshop on Theoretical Foundations and Applications of Deep Generative Models.

Dhruv Madeka, Lucas Swiniarski, Dean Foster, Leo Razoumov, Kari Torkkola, Ruofeng Wen (2018). [Sample Path Generation for Probabilistic Demand Forecasting](https://milets18.github.io/papers/milets18_paper_14.pdf). 4th Workshop on Mining and Learning from Time Series (MiLeTS).

Tim Radtke (2018). [Understanding the Negative Binomial Distribution](https://minimizeregret.com/post/2018/01/04/understanding-the-negative-binomial-distribution/). Blog post on Minimize Regret. 

Tim Radtke (2017). [Detection of Abnormal Zero-Sequences in Count Time Series](https://minimizeregret.com/post/2017/10/22/detection-abnormal-zero-sequences-count-time-series/). Blog post on Minimize Regret.

Matthias Seeger, David Salinas, Valentin Flunkert (2016). [Bayesian Intermittent Demand Forecasting for Large Inventories](https://papers.nips.cc/paper/6313-bayesian-intermittent-demand-forecasting-for-large-inventories). Advances in Neural Information Processing Systems 29 (NIPS 2016).

Ralph D. Snyder, J. Keith Ord, Adrian Beaumont (2012). [Forecasting the intermittent demand for slow-moving inventories: A modelling approach](https://www.researchgate.net/publication/251527888). International Journal of Forecasting 28, 485-496.

Thiyanga Talagala, Rob J Hyndman, George Athanasopoulos (2018). [Meta-learning how to forecast time series](https://robjhyndman.com/publications/fforms/). Monash University Working Paper 6/18.

Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, Dhruv Madeka (2018). [A Multi-Horizon Quantile Recurrent Forecaster](https://arxiv.org/abs/1711.11053). 31st Conference on Neural Information Processing Systems (NIPS 2017), Time Series Workshop.

Achim Zeileis, Christian Kleiber, Simon Jackman (2017). [Regression Models for Count Data in R](https://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf). Vignette on the Comprehensive R Archive Network.
